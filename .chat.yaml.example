# Make changes, and then save the file as: .chat.yaml
# Supplied command line arguments override settings here, allowing you to
# store common arguments like server, yet pass --model on the fly.
# NOTE: The values below should match the full options name (see --help) with
#       underscores instead of dashes. (eg: context_window not context-window)
#
# Interesting Note: history_dir contains *everything* about your experience. You
# therefore can change this on a whim to have a new history/experience with your
# LLM, and then change it back to continue past-conversations. Or, delete this
# directory when finished to remove any trace of your conversations.
chat:
  model: gemma3:27b                       (heavy-weight LLM, your main go-to)
  pre_llm: gemma3:1b                      (light-weight LLM, used for metadata tagging)
  embedding_llm: nomic-embed-text         (embeddings for RAG vector database work)
  history_dir:                            /some/writable/path or leave blank for default
  history_matches: 5                      (how many matches to allow from the RAG: 5 from user's RAG and 5 from AIs)
  history_session: 5                      (how many turns of history to provide. Thats 15 from all sources by default)
  llm_server: https://api.openai.com/v1   (an OpenAI compatible API server address. If using Ollama locally that would be: http://localhost:11434/v1)
  pre_server:                             (the server for which is hosting the pre_llm model, or blank to use llm_server)
  embedding_server:                       (the server for which is hosting the embedding_llm model, or blank to use llm_server)
  api_key: YOUR API KEY                   (your assigned API key, or empty if using local Ollama server)
  time_zone: 'America/Denver'             (currenlty not used in the system prompts, easy to implement though: {date_time})
  light_mode: False                       (set to true if you use a high luminance terminal background)
  context_window: 4192                    (ask the server to use this context window size. Sometimes can be ignore)
  name: your assistants name              (name your assistant/story-teller)
  assitant_mode: True                     (use non-story telling prompts. Useful if you're using this chat script as a tool instead of a story teller)
  debug: False

# Ollama server switches of note:
# I have good experiences with the above models running locally at 25+ tokens/s (Mac Studio 512GB),
# using the following Ollama environment variables set:
#
# OLLAMA_CONTEXT_LENGTH=32768
# OLLAMA_KEEP_ALIVE=24h
# OLLAMA_MAX_LOADED_MODELS=3
# OLLAMA_KV_CACHE_TYPE=f16
# OLLAMA_FLASH_ATTENTION=true
